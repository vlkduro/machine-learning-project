{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e5bfb37",
   "metadata": {},
   "source": [
    "## AI28 - Projet - Prédiction du niveau de revenus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e2db09",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "a8ac31df",
   "metadata": {},
   "source": [
    "# pip install ucimlrepo"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "001b1097",
   "metadata": {},
   "source": [
    "# OS\n",
    "import sys\n",
    "\n",
    "\n",
    "# WARNINGS\n",
    "import warnings\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('error', category=DeprecationWarning)\n",
    "\n",
    "# NUMPY\n",
    "import numpy as np\n",
    "\n",
    "# STATS\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm, skew\n",
    "\n",
    "# MATPLOTLIB\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# plt.style.use('fivethirtyeight') \n",
    "\n",
    "params = {'axes.labelsize': 8, # 12\n",
    "          'font.size': 8, # 12\n",
    "          'legend.fontsize': 8, # 12\n",
    "          'xtick.labelsize': 8, # 10\n",
    "          'ytick.labelsize': 8, # 10\n",
    "          'text.usetex': True,\n",
    "          'figure.figsize': (10, 8)}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "\n",
    "# PANDAS\n",
    "import pandas as pd \n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None) \n",
    "\n",
    "# SEABORN \n",
    "import seaborn as sns\n",
    "# sns.set_context(\"poster\")\n",
    "# sns.set_style(\"ticks\")\n",
    "\n",
    "# SCHIKIT-LEARN: PRE-PROCESSING\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder # Encodage des variables catégorielles ordinales\n",
    "from sklearn.preprocessing import LabelBinarizer, OneHotEncoder # Encodage des variables catégorielles nominales\n",
    "from sklearn.preprocessing import StandardScaler # Standardisation des variables numériques\n",
    "from sklearn.preprocessing import MinMaxScaler # Normalisation MinMax\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import SimpleImputer # Imputation\n",
    "from sklearn.impute import KNNImputer \n",
    "\n",
    "\n",
    "# SCHIKIT-LEARN: MODELES\n",
    "from sklearn import linear_model # Classe Modèle linéaire \n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso \n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR \n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# SCHIKIT-LEARN: VALIDATION CROISEE + OPTIMISATION\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score # Validation croisée pour comparaison entre modèles\n",
    "from sklearn.model_selection import validation_curve # Courbe de validation: visualiser les scores lors du choix d'un hyperparamétre\n",
    "from sklearn.model_selection import GridSearchCV # Tester plusieurs hyperparamètres\n",
    "from sklearn.model_selection import RandomizedSearchCV # Tester plusieurs hyperparamètres\n",
    "from sklearn.model_selection import learning_curve # Courbe d'apprentissage: visualisation des scores du train et du validation sets en fonction des quantités des données\n",
    "import optuna\n",
    " \n",
    "## EVALUATION\n",
    "\n",
    "from sklearn.metrics import r2_score, accuracy_score, recall_score, precision_score  # Coefficient R2\n",
    "from sklearn.metrics import mean_absolute_error #  MAE\n",
    "from sklearn.metrics import mean_squared_error  # RMSE\n",
    "from sklearn.metrics import median_absolute_error # Erreur de la médiane absolue\n",
    "\n",
    "# SCHIKIT-LEARN: PIPELINE AND TRANSFORMATEUR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "# SOURCE DATA\n",
    "from ucimlrepo import fetch_ucirepo "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6ce14a4a",
   "metadata": {},
   "source": [
    "# Utilisation des commandes fournies par UCI\n",
    "\n",
    "# Télécharger le jeu de données \n",
    "data_adult = fetch_ucirepo(id=2) "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cdfbbc57",
   "metadata": {},
   "source": [
    "# data (as pandas dataframes) \n",
    "x = data_adult.data.features # variables explicatives\n",
    "y = data_adult.data.targets # variables cibles"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3347fb27",
   "metadata": {},
   "source": [
    "# metadata \n",
    "print(data_adult.metadata) "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e5866137",
   "metadata": {},
   "source": [
    "# variable information \n",
    "print(data_adult.variables) "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6cbde9f3",
   "metadata": {},
   "source": [
    "## Analyse exploratoire des données\n",
    "\n",
    "Pour réaliser l'AED, nous utilisons les données des fichiers téléchargés depuis le site web."
   ]
  },
  {
   "cell_type": "code",
   "id": "7dab83b8",
   "metadata": {},
   "source": [
    "df_adult = pd.read_csv(\"./Data/adult.data\", header=None)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a17d559b",
   "metadata": {},
   "source": [
    "isinstance(df_adult, pd.DataFrame)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "773202c2",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8dd95f63",
   "metadata": {},
   "source": [
    "Le fichier ne contient pas les noms des colonnes. Nous les ajoutons dans le dataframe adult."
   ]
  },
  {
   "cell_type": "code",
   "id": "47221cbf",
   "metadata": {},
   "source": [
    "adult_columns = data_adult.variables['name']\n",
    "\n",
    "df_adult.columns = adult_columns\n",
    "\n",
    "df_adult.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "107289ca",
   "metadata": {},
   "source": [
    "df_adult.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6d032d69",
   "metadata": {},
   "source": [
    "df_adult.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3f021163",
   "metadata": {},
   "source": [
    "df_adult.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b18aed3e",
   "metadata": {},
   "source": [
    "df_adult.isna().sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "136d56a2",
   "metadata": {},
   "source": [
    "df_adult = df_adult.drop_duplicates()\n",
    "df_adult[df_adult.duplicated()]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f1a4235f",
   "metadata": {},
   "source": [
    "df_adult.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ec6c3644",
   "metadata": {},
   "source": [
    "Problème : La description des données indique qu'elles contiennent des valeurs inconnues. Ces valeurs ne sont pas en NaN ou null. Nous pouvons déjà remarquer la présence de '?' en leur lieu et place.\n",
    "\n",
    "## Traitement des données"
   ]
  },
  {
   "cell_type": "code",
   "id": "4f71c629",
   "metadata": {},
   "source": [
    "df_adult = df_adult.replace('\\?', np.nan, regex=True)\n",
    "sns.heatmap(df_adult.isna(), cbar=False, cmap='viridis')\n",
    "print(f\"Nombre de valeurs manquantes : {df_adult.isna().sum()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6c06b28a72dceb89",
   "metadata": {},
   "source": [
    "# On remplace les données manquantes par le mode les plus frequentes (si elles depassent les 85%)\n",
    "wrkclass_proportion = df_adult[\"workclass\"].value_counts(dropna=False) / df_adult.shape[0]\n",
    "print(\"workclass : \", wrkclass_proportion.idxmax(), f\"{wrkclass_proportion.max() * 100:.2f}% ({len(wrkclass_proportion)} valeurs uniques)\")\n",
    "occupation_proportion = df_adult[\"occupation\"].value_counts(dropna=False) / df_adult.shape[0]\n",
    "print(\"occupation : \", occupation_proportion.idxmax(), f\"{occupation_proportion.max() * 100:.2f}% ({len(occupation_proportion)} valeurs uniques)\")\n",
    "native_country_proportion = df_adult[\"native-country\"].value_counts(dropna=False) / df_adult.shape[0]\n",
    "print(\"native_country : \", native_country_proportion.idxmax(), f\"{native_country_proportion.max() * 100:.2f}% ({len(native_country_proportion)} valeurs uniques)\")\n",
    "income_proportion = df_adult[\"income\"].value_counts(dropna=False) / df_adult.shape[0]\n",
    "print(\"income : \", income_proportion.idxmax(), f\"{income_proportion.max() * 100:.2f}% ({len(income_proportion)} valeurs uniques)\")\n",
    "\n",
    "# Pour l'instant on remplit les valeurs manquantes par les valeurs les plus fréquentes.\n",
    "for col in [\"workclass\", \"occupation\", \"native-country\"]:\n",
    "    df_adult[col].fillna(df_adult[col].mode()[0], inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1add214f46def1ba",
   "metadata": {},
   "source": [
    "categorical_columns = df_adult.select_dtypes(include=['object']).columns\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "df_adult_enco = df_adult.copy()\n",
    "# On retire les colonnes inutiles\n",
    "# print(df_adult_enco.columns)\n",
    "# df_adult_enco = df_adult_enco.drop(columns=[\"education-num\"])\n",
    "\n",
    "# On encode les variables catégorielles ordinales\n",
    "for col in categorical_columns:\n",
    "    df_adult_enco[col] = ordinal_encoder.fit_transform(df_adult[[col]])\n",
    "\n",
    "print(df_adult_enco.head())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d61c0a4ba40038da",
   "metadata": {},
   "source": [
    "> On pourrait normaliser les variables numériques, mais pour l'instant on ne le fait pas. On va se concentrer sur la création du modèle de prédiction.\n",
    "\n",
    "## Modele de prediction\n",
    "\n",
    "### Setup\n",
    "\n",
    "> On pourrait faire une pipeline de pretraitement pour chaque pipeline (LabelEncoder, normalisation et drop des values NaN)"
   ]
  },
  {
   "cell_type": "code",
   "id": "aa55e6c1908508db",
   "metadata": {},
   "source": [
    "# Creation de différents sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_adult_enco.drop(columns=[\"income\"]), df_adult_enco[\"income\"], test_size=0.2, random_state=69)\n",
    "\n",
    "# On normalise les variables quantitatives sur les données de train (StandardScaler pour l'instant)\n",
    "numerical_columns = df_adult.select_dtypes(include=['int64', 'float64']).columns\n",
    "numerical_scaler = StandardScaler()\n",
    "numerical_scaler.fit(X_train[numerical_columns])\n",
    "X_train[numerical_columns] = numerical_scaler.transform(X_train[numerical_columns])\n",
    "X_test[numerical_columns] = numerical_scaler.transform(X_test[numerical_columns])\n",
    "\n",
    "def eval_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f'=== Evaluation du modèle {model.__class__.__name__} ===')\n",
    "    print(f\"Accuracy : {accuracy_score(y_test, y_pred):.4f} (the higher the better)\")\n",
    "    print(f\"Recall : {recall_score(y_test, y_pred):.4f} (the higher the better) (best for us)\")\n",
    "    print(f\"R2 : {r2_score(y_test, y_pred):.4f} (the higher the better)\")\n",
    "    print(f\"MAE : {mean_absolute_error(y_test, y_pred):.4f} (the lower the better)\")\n",
    "    print(f\"RMSE : {mean_squared_error(y_test, y_pred):.4f} (the lower the better)\")\n",
    "    return y_pred"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cec6205532cb2e1e",
   "metadata": {},
   "source": [
    "### Arbre de décision"
   ]
  },
  {
   "cell_type": "code",
   "id": "faf2e7624837066c",
   "metadata": {},
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def opt_decision_tree(trial):\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 32)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 4)\n",
    "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
    "    criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "    return DecisionTreeClassifier(\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        criterion=criterion\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ce476f8a0cf7891a",
   "metadata": {},
   "source": [
    "> Pas de tres bonnes performances, on va essayer avec un autre modèle.\n",
    "\n",
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "id": "52405f1f0bcfb978",
   "metadata": {},
   "source": [
    "\n",
    "def opt_random_forest(trial):\n",
    "    rf_max_depth = trial.suggest_int('rf_max_depth', 2, 32)\n",
    "    rf_min_samples_split = trial.suggest_int('rf_min_samples_split', 2, 10)\n",
    "    rf_min_samples_leaf = trial.suggest_int('rf_min_samples_leaf', 1, 4)\n",
    "    rf_max_features = trial.suggest_categorical('rf_max_features', ['sqrt', 'log2'])\n",
    "    rf_criterion = trial.suggest_categorical('rf_criterion', ['gini', 'entropy'])\n",
    "    rf_n_estimators = trial.suggest_int('rf_n_estimators', 50, 200)\n",
    "    return RandomForestClassifier(\n",
    "        max_depth=rf_max_depth,\n",
    "        min_samples_split=rf_min_samples_split,\n",
    "        min_samples_leaf=rf_min_samples_leaf,\n",
    "        max_features=rf_max_features,\n",
    "        criterion=rf_criterion,\n",
    "        n_estimators=rf_n_estimators\n",
    "      )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Regression logistique",
   "id": "6aa0b26492f0ea26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def opt_logistic_regression(trial):\n",
    "    solver = trial.suggest_categorical('solver', ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'])\n",
    "\n",
    "    # Filter penalties based on the solver\n",
    "    penalty = \"l2\"  # Default penalty if an error occurs\n",
    "    if solver in ['newton-cg', 'lbfgs', 'sag']:\n",
    "        penalty = trial.suggest_categorical('penalty', ['l2', None])\n",
    "    elif solver == 'liblinear':\n",
    "        penalty = trial.suggest_categorical('penalty-liblinear', ['l1', 'l2'])\n",
    "    elif solver == 'saga':\n",
    "        penalty = trial.suggest_categorical('penalty-saga', ['l1', 'l2', 'elasticnet', None])\n",
    "\n",
    "    C = trial.suggest_float('C', 0.01, 10.0, log=True)\n",
    "    max_iter = trial.suggest_int('max_iter', 50, 500)\n",
    "    return linear_model.LogisticRegression(\n",
    "        penalty=penalty,\n",
    "        C=C,\n",
    "        solver=solver,\n",
    "        max_iter=max_iter\n",
    "    )"
   ],
   "id": "e7c1331e8f9d6309",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1a3031cdc514bae7",
   "metadata": {},
   "source": [
    "### AdaBoosting"
   ]
  },
  {
   "cell_type": "code",
   "id": "eba4e21b00687def",
   "metadata": {},
   "source": [
    "def opt_adaboost(trial):\n",
    "    base_estimator = trial.suggest_categorical('base_estimator', ['DecisionTree', 'RandomForest'])\n",
    "    base_estimator_obj = None\n",
    "    if base_estimator == 'DecisionTree':\n",
    "        base_estimator_obj = opt_decision_tree(trial)\n",
    "    elif base_estimator == 'RandomForest':\n",
    "        base_estimator_obj = opt_random_forest(trial)\n",
    "\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3)\n",
    "\n",
    "    return AdaBoostClassifier(\n",
    "        estimator=base_estimator_obj,\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d3f9ecc64667cb38",
   "metadata": {},
   "source": [
    "### Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "id": "8fa4715e1c5204c3",
   "metadata": {},
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "def opt_gradient_boosting(trial):\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 32)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 4)\n",
    "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
    "    criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3)\n",
    "    return XGBClassifier(\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        criterion=criterion,\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1fdc4d01bc8789a4",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "id": "60931346aa26bf34",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "def opt_stacking(trial):\n",
    "    rf_max_depth = trial.suggest_int('rf_max_depth', 2, 32)\n",
    "    rf_min_samples_split = trial.suggest_int('rf_min_samples_split', 2, 10)\n",
    "    rf_min_samples_leaf = trial.suggest_int('rf_min_samples_leaf', 1, 4)\n",
    "    rf_max_features = trial.suggest_categorical('rf_max_features', ['sqrt', 'log2'])\n",
    "    rf_criterion = trial.suggest_categorical('rf_criterion', ['gini', 'entropy'])\n",
    "    rf_n_estimators = trial.suggest_int('rf_n_estimators', 50, 200)\n",
    "\n",
    "    xgb_max_depth = trial.suggest_int('xgb_max_depth', 2, 32)\n",
    "    xgb_min_samples_split = trial.suggest_int('xgb_min_samples_split', 2, 10)\n",
    "    xgb_min_samples_leaf = trial.suggest_int('xgb_min_samples_leaf', 1, 4)\n",
    "    xgb_max_features = trial.suggest_categorical('xgb_max_features', ['sqrt', 'log2'])\n",
    "    xgb_criterion = trial.suggest_categorical('xgb_criterion', ['gini', 'entropy'])\n",
    "    xgb_n_estimators = trial.suggest_int('xgb_n_estimators', 50, 200)\n",
    "\n",
    "    return StackingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', RandomForestClassifier(\n",
    "                max_depth=rf_max_depth,\n",
    "                min_samples_split=rf_min_samples_split,\n",
    "                min_samples_leaf=rf_min_samples_leaf,\n",
    "                max_features=rf_max_features,\n",
    "                criterion=rf_criterion,\n",
    "                n_estimators=rf_n_estimators\n",
    "            )),\n",
    "            ('xgb', XGBClassifier(\n",
    "                max_depth=xgb_max_depth,\n",
    "                min_samples_split=xgb_min_samples_split,\n",
    "                min_samples_leaf=xgb_min_samples_leaf,\n",
    "                max_features=xgb_max_features,\n",
    "                criterion=xgb_criterion,\n",
    "                n_estimators=xgb_n_estimators\n",
    "            ))\n",
    "        ],\n",
    "        final_estimator=XGBClassifier(random_state=69)\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Optimisation des hyperparamètres avec `optuna`",
   "id": "f90251a4bf92921"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def objective(trial):\n",
    "    # Invoke suggest methods of a Trial object to generate hyperparameters.\n",
    "    classifier_name = trial.suggest_categorical('classifier', ['DecisionTree', 'RandomForest', 'LogisticRegression', 'AdaBoosting', 'GradientBoosting'])#, 'Stacking'])\n",
    "    # Create a classifier object based on the suggested hyperparameters.\n",
    "    classifier_obj = None\n",
    "    if classifier_name == 'DecisionTree':\n",
    "        classifier_obj = opt_decision_tree(trial)\n",
    "    elif classifier_name == 'RandomForest':\n",
    "        classifier_obj = opt_random_forest(trial)\n",
    "    elif classifier_name == 'LogisticRegression':\n",
    "        classifier_obj = opt_logistic_regression(trial)\n",
    "    elif classifier_name == 'AdaBoosting':\n",
    "        classifier_obj = opt_adaboost(trial)\n",
    "    elif classifier_name == 'GradientBoosting':\n",
    "        classifier_obj = opt_gradient_boosting(trial)\n",
    "    elif classifier_name == 'Stacking':\n",
    "        classifier_obj = opt_stacking(trial)\n",
    "\n",
    "    classifier_obj.fit(X_train, y_train)\n",
    "    y_pred = classifier_obj.predict(X_test)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    # return recall, precision\n",
    "    return recall\n",
    "\n",
    "# study = optuna.create_study(directions=[\"maximize\", \"maximize\"])\n",
    "# On tolère 1 chance sur 1.000.000 de se tromper\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"Adult Income Prediction\")\n",
    "study.optimize(objective, n_trials=150)\n",
    "print(\"Best hyperparameters: \", study.best_params)\n",
    "\n",
    "best_model = None\n",
    "if study.best_params['classifier'] == 'DecisionTree':\n",
    "    best_model = opt_decision_tree(optuna.trial.FixedTrial(study.best_params))\n",
    "elif study.best_params['classifier']  == 'RandomForest':\n",
    "    best_model = opt_random_forest(optuna.trial.FixedTrial(study.best_params))\n",
    "elif study.best_params['classifier']  == 'LogisticRegression':\n",
    "    best_model = opt_logistic_regression(optuna.trial.FixedTrial(study.best_params))\n",
    "elif study.best_params['classifier']  == 'AdaBoosting':\n",
    "    best_model = opt_adaboost(optuna.trial.FixedTrial(study.best_params))\n",
    "elif study.best_params['classifier']  == 'GradientBoosting':\n",
    "    best_model = opt_gradient_boosting(optuna.trial.FixedTrial(study.best_params))\n",
    "elif study.best_params['classifier']  == 'Stacking':\n",
    "    best_model = opt_stacking(optuna.trial.FixedTrial(study.best_params))\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "eval_model(best_model, X_test, y_test)\n"
   ],
   "id": "47798d934fb9f931",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_adult_enco.drop(columns=[\"income\"]), df_adult_enco[\"income\"], test_size=0.2)\n",
    "    # On normalise les variables quantitatives sur les données de train (StandardScaler pour l'instant)\n",
    "    numerical_columns = df_adult.select_dtypes(include=['int64', 'float64']).columns\n",
    "    numerical_scaler = StandardScaler()\n",
    "    numerical_scaler.fit(X_train[numerical_columns])\n",
    "    X_train[numerical_columns] = numerical_scaler.transform(X_train[numerical_columns])\n",
    "    X_test[numerical_columns] = numerical_scaler.transform(X_test[numerical_columns])\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "    print(f\"\\n=== Evaluation num {i+1} du modèle ===\")\n",
    "    eval_model(best_model, X_test, y_test)"
   ],
   "id": "6dcb8886c8e7b9ca",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
